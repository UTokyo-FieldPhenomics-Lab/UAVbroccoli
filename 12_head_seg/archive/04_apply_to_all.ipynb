{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 - apply to all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisenet import engine\n",
    "import config\n",
    "from config import *\n",
    "from utils import *\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model, batch, device='cpu'):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        model ([model]): [deeplab model]\n",
    "        img ([list]): [batch of images]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        batch = batch.to(device)\n",
    "        # print(img)\n",
    "        pred, *_= model(batch)\n",
    "        # print(pred.shape)\n",
    "        masks = pred.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
    "        masks = masks.argmax(3).reshape(-1, 128, 128, 1)\n",
    "        return np.array(masks, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bisenet_engine = engine.bisenet_engine(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = bisenet_engine.model\n",
    "model_weight = torch.load(config.model_weight)\n",
    "model_weight = model_weight[\"model_state_dict\"]\n",
    "model.load_state_dict(model_weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Load InfoMAP and prepare batch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_map = idp.jsonfile.read_json(r\"Z:\\hwang_Pro\\data\\2022_tanashi_broccoli\\12_head_segment\\broccoli_autumn21-20220405_0.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio as io\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "transform = A.Compose([\n",
    "        A.Resize(128, 128),\n",
    "        # A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:/hwang_Pro/data/2022_tanashi_broccoli/12_head_segment/broccoli_autumn21-20220405_0/2_DJI_0355.png\n"
     ]
    }
   ],
   "source": [
    "values = info_map['2_DJI_0355']\n",
    "croped_image_path = values['cropedImagePath']\n",
    "print(croped_image_path)\n",
    "coords = values['headCoordOnCroppedImage']\n",
    "base = 75\n",
    "img = io.imread(croped_image_path)\n",
    "h, w, _ = img.shape\n",
    "one_mask = np.zeros((h, w, 1), dtype=np.uint8)\n",
    "\n",
    "# convert to numpy array\n",
    "points = np.array(coords, dtype=np.int32)\n",
    "# calcualte x0, y0, x1, y1\n",
    "y0 = points[:, 1] - base\n",
    "x0 = points[:, 0] - base\n",
    "y1 = points[:, 1] + base\n",
    "x1 = points[:, 0] + base\n",
    "# avoid minus number\n",
    "y0[y0 < 0] = 0\n",
    "x0[x0 < 0] = 0\n",
    "y1[y1 > h] = h\n",
    "x1[x1 > w] = w\n",
    "\n",
    "bboxs = np.array(list(zip(y0, x0, y1, x1)))\n",
    "# get sub images\n",
    "batch = []\n",
    "for idx, (y_0, x_0, y_1, x_1) in enumerate(bboxs):\n",
    "    sub_img = img[y_0:y_1, x_0:x_1, :3]\n",
    "    sub_img = transform(image=sub_img)['image']\n",
    "    batch.append(sub_img)\n",
    "\n",
    "batch = torch.stack(batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Model Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst = predict_batch(model, batch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "for box, mask in zip(bboxs, rst):\n",
    "    # print(np.unique(mask))\n",
    "    y0, x0, y1, x1 = box\n",
    "    # print(mask, mask.shape) \n",
    "    mask = transforms.Resize((y1-y0, x1-x0))(torch.tensor(mask).permute((2,0,1)))*255\n",
    "    \n",
    "    mask[mask > 50] = 255\n",
    "    mask[mask <= 50] = 100\n",
    "    # print('masks:', np.unique(mask))\n",
    "    mask = np.asarray(mask).transpose((1,2,0))\n",
    "    \n",
    "    # print(mask.shape)\n",
    "    one_mask[y0:y1, x0:x1, :] = mask\n",
    "\n",
    "    # save visualization\n",
    "    io.imsave('./test.png', one_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uavb2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
